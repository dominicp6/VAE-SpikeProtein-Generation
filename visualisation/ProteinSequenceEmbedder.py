import os
import umap.umap_ as umap  # make sure that you install "umap-learn" not "umap"
import matplotlib.pyplot as plt
from amino_acid_encoding import ProteinSequenceEncoder
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

default_hyperparameters = \
    {'PCA': {'whiten': False},
     'tSNE': {'perplexity': 30, 'learning_rate': 200, 'n_iter': 1000,
              'metric': 'cosine', 'PCA_dim': 50, 'whiten': False},
     'UMAP': {'n_neighbors': 15, 'metric': 'cosine', 'learning_rate': 1, 'seed': 42}}


class ProteinSequenceEmbedder:

    def __init__(self,
                 encoding_type,
                 embedding_type,
                 encodings_directory=os.path.join("..", "data", "encodings"),
                 data_directory=os.path.join("..", "data", "spike_protein_sequences")):

        assert embedding_type == 'PCA' \
               or embedding_type == 'UMAP' \
               or embedding_type == 'tSNE', "embedding type must be either PCA, UMAP or tSNE"

        self.encoding_type = encoding_type
        self.embedding_type = embedding_type
        self.encodings_directory = encodings_directory
        self.data_directory = data_directory

        self.embedding_data = {'embeddings': [],
                               'frequencies': [],
                               'labels': [],
                               'meta_data': []}

    def _encode_sequences(self, infile, mask=None):
        """
        Encodes aligned sequences from a fasta file.

        Valid encoding types are:
        'One_hot', 'One_hot_6_bit', 'Binary_5_bit', 'Hydrophobicity_matrix', 'Meiler_parameters', 'Acthely_factors',
        'PAM250', 'BLOSUM62', 'Miyazawa_energies', 'Micheletti_potentials', 'AESNN3', 'ANN4D', 'ProtVec'

        Note:
        If a mask is required, then it should be generated by running the get_epitope_mask function from epitope_mask.py

        :param infile: the fasta file of aligned sequences.
        :param mask: an epitope mask to apply to the encoding (optional).
        :return:
        frequencies       - the number of times that each sequence appeared (used during visualisation)
        encoded_sequences - the encoded amino acid sequences
        """
        encoder = ProteinSequenceEncoder(encoding_type=self.encoding_type,
                                         data_directory=self.data_directory,
                                         encodings_directory=self.encodings_directory,
                                         mask=mask)

        descriptors, encoded_sequences = encoder.encode_from_fasta_file(infile,
                                                                        f'1_in_500_encoded_{self.encoding_type}.txt')

        return descriptors, encoded_sequences

    @staticmethod
    def embed_with_PCA(encoded_sequences, hyperparameters=default_hyperparameters['PCA']):
        """
        Embeds a list of encoded sequences down to two dimensions using PCA.
        """
        reducer = PCA(n_components=2, whiten=hyperparameters['whiten'])
        sequence_embeddings = reducer.fit_transform(encoded_sequences)
        explained_variance = reducer.explained_variance_ratio_
        return sequence_embeddings, explained_variance

    @staticmethod
    def embed_with_tSNE(encoded_sequences, hyperparameters=default_hyperparameters['tSNE']):
        """
        Embeds a list of encoded sequences down to two dimensions using tSNE.
        """
        pca = PCA(n_components=hyperparameters['PCA_dim'], whiten=hyperparameters['whiten'])
        pca_reduced_embeddings = pca.fit_transform(encoded_sequences)
        reducer = TSNE(init='pca',
                       perplexity=hyperparameters['perplexity'],
                       learning_rate=hyperparameters['learning_rate'],
                       n_iter=hyperparameters['n_iter'],
                       metric=hyperparameters['metric']
                       )
        sequence_embeddings = reducer.fit_transform(pca_reduced_embeddings)
        return sequence_embeddings

    @staticmethod
    def embed_with_UMAP(encoded_sequences, hyperparameters=default_hyperparameters['UMAP']):
        """
        Embeds a list of encoded sequences down to two dimensions using UMAP.
        """
        reducer = umap.UMAP(metric=hyperparameters['metric'],
                            random_state=hyperparameters['seed'],
                            n_neighbors=hyperparameters['n_neighbors'],
                            learning_rate=hyperparameters['learning_rate'])
        sequence_embeddings = reducer.fit_transform(encoded_sequences)
        return sequence_embeddings

    def plot_embedding_map(self, markerSize=5):
        """
        Plots a 2D map from a list of embeddings and their associated sequence frequency.

        :param markerSize: relative point markerSize
        """

        marker_sizes = [frequency * markerSize for frequency in self.embedding_data['frequencies']]

        # plot
        plt.scatter(self.embedding_data['embeddings'][:, 0],
                    self.embedding_data['embeddings'][:, 1],
                    s=marker_sizes, alpha=0.25)

        # title, labels and aspect ratio
        plt.gca().set_aspect('equal', 'datalim')
        plt.gcf().set_size_inches(12, 12)
        plt.title('2D embeddings of SARS-COV19 spike proteins', fontsize=20)
        plt.xlabel(f'{self.embedding_type} 1')
        plt.ylabel(f'{self.embedding_type} 2')

        # annotate with meta data
        plt.gca().text(0.95, 0.95, '\n'.join(self.embedding_data['meta_data']),
                       horizontalalignment='right', verticalalignment='top',
                       transform=plt.gca().transAxes)

        plt.show()

    @staticmethod
    def _parse_sequence_descriptors(descriptors):
        frequencies = []
        dates = []
        labels = []
        for descriptor in descriptors:
            split_descriptor = descriptor.split('|')
            try:
                labels.append(split_descriptor[2])
                dates.append(split_descriptor[1])
                frequencies.append(int(split_descriptor[0]))
            except:
                try:
                    labels.append(None)
                    dates.append(split_descriptor[1])
                    frequencies.append(int(split_descriptor[0]))
                except:
                    labels.append(None)
                    dates.append(None)
                    frequencies.append(int(split_descriptor[0]))

        return frequencies, dates, labels

    def embed_sequences(self, infile, hyperparameters, mask):

        # check hyperparameters
        hyperparameters = self.validate_hyperparameters(hyperparameters)

        # encode
        descriptors, encoded_sequences = self._encode_sequences(infile, mask)

        # get frequency, median date and variant label from its descriptor
        frequencies, dates, labels = self._parse_sequence_descriptors(descriptors)

        if mask is not None:
            mask_present = True
        else:
            mask_present = False

        meta_data = [self.embedding_type, f'encoding: {self.encoding_type}', f'mask: {mask_present}']
        meta_data.extend([f'{param}: {value}' for param, value in hyperparameters.items()])

        if self.embedding_type == 'PCA':
            sequence_embeddings, explained_variance = self.embed_with_PCA(encoded_sequences, hyperparameters)
            meta_data.extend([f'{round(variance, 3)}' for variance in explained_variance])
        elif self.embedding_type == 'tSNE':
            sequence_embeddings = self.embed_with_tSNE(encoded_sequences, hyperparameters)
        elif self.embedding_type == 'UMAP':
            sequence_embeddings = self.embed_with_UMAP(encoded_sequences, hyperparameters)
        else:
            raise Exception('Invalid method. Must be PCA, tSNE or UMAP.')

        self.embedding_data['embeddings'] = sequence_embeddings
        self.embedding_data['frequencies'] = frequencies
        self.embedding_data['dates'] = dates
        self.embedding_data['meta_data'] = meta_data

        return self.embedding_data

    def validate_hyperparameters(self, hyperparameter_dict):

        if hyperparameter_dict is None:
            hyperparameter_dict = default_hyperparameters[self.embedding_type]
        else:
            assert set(hyperparameter_dict.keys()).issubset(default_hyperparameters[self.embedding_type]), \
                f"Invalid entries for hyperparameters. " \
                f"Permitted dict keys are {default_hyperparameters[self.embedding_type]}"
            unspecified_hyperparameters = \
                set(default_hyperparameters[self.embedding_type]).difference(hyperparameter_dict.keys())
            for hyperparameter in unspecified_hyperparameters:
                hyperparameter_dict[hyperparameter] = default_hyperparameters[self.embedding_type][hyperparameter]

        return hyperparameter_dict

    def generate_embedding_map(self,
                               infile,
                               hyperparameters=None,
                               markerSize=1,
                               mask=None):
        """
        Plots a 2D representation of the dimensionality-reduced embeddings of the encoded sequences in a fasta database.

        :param hyperparameters: Dictionary of hyperparameters for the embedding algorithm.
        :param infile: a database of aligned fasta sequences.
        :param markerSize:  relative point markerSize.
        :param mask:        An epitope mask to be applied to the encoded sequences (optional).
        """

        self.embed_sequences(infile=infile, hyperparameters=hyperparameters, mask=mask)
        self.plot_embedding_map(markerSize=markerSize)
